# Instant Coffee - æŠ€æœ¯è§„æ ¼è¯´æ˜ä¹¦ (Spec v0.3)

**é¡¹ç›®åç§°**: Instant Coffee (é€Ÿæº¶å’–å•¡)
**ç‰ˆæœ¬**: v0.3 - Agent LLM è°ƒç”¨ + Tools ç³»ç»Ÿ
**æ—¥æœŸ**: 2026-01-31
**æ–‡æ¡£ç±»å‹**: Technical Specification Document (TSD)

---

## æ–‡æ¡£å˜æ›´å†å²

| ç‰ˆæœ¬ | æ—¥æœŸ | å˜æ›´å†…å®¹ | ä½œè€… |
|------|------|---------|------|
| v0.1 | 2025-01-30 | åˆå§‹ç‰ˆæœ¬ï¼ŒCLI + åç«¯æ ¸å¿ƒåŠŸèƒ½ | Interview |
| v0.2 | 2025-01-30 | Web å‰ç«¯ + æ‰§è¡Œæµå¯è§†åŒ– + Planner | Planning |
| v0.3 | 2025-01-31 | Agent LLM è°ƒç”¨ + Tools ç³»ç»Ÿ | Implementation |
| v0.3.1 | 2026-01-31 | ä¿®è®¢æµå¼/å·¥å…·/å®‰å…¨è¾¹ç•Œç­‰ç»†èŠ‚ | Review |

---

## è®¾è®¡å†³ç­–è®¿è°ˆè®°å½• (2026-01-31 ä¿®è®¢)

æœ¬èŠ‚è®°å½•äº† spec-03 å®ç°å‰çš„å…³é”®è®¾è®¡å†³ç­–è®¿è°ˆç»“æœã€‚

### æ ¸å¿ƒå†³ç­–

| é—®é¢˜ | å†³ç­– | è¯´æ˜ |
|------|------|------|
| æµå¼è¾“å‡ºç­–ç•¥ | åç«¯æŒç»­æµå¼äº‹ä»¶ï¼Œå‰ç«¯å¯é˜¶æ®µå±•ç¤º | åç«¯æŒç»­å‘å°„äº‹ä»¶ï¼Œå‰ç«¯é»˜è®¤åªæ˜¾ç¤ºé˜¶æ®µå®Œæˆä¿¡æ¯ï¼Œå¯åˆ‡æ¢å®æ—¶ |
| Token æˆæœ¬æ˜¾ç¤º | ä¼šè¯ç»“æŸæ±‡æ€» | å‡å°‘åˆ·å±å¹²æ‰°ï¼Œä¼šè¯ç»“æŸæ—¶ç»Ÿä¸€æ˜¾ç¤ºï¼ˆå¿…è¦æ—¶å¯å®æ—¶ï¼‰ |
| ä¸Šä¸‹æ–‡ç®¡ç† | LLM ç®¡ç† + å¯æ’æ‹”æ‘˜è¦ | ä»¥ LLM ä¸ºä¸»ï¼Œé¢„ç•™æ‘˜è¦/è£å‰ªç­–ç•¥ä»¥åº”å¯¹è¶…é•¿ä¸Šä¸‹æ–‡ |
| HTML æå– | æ ‡è®°ä¼˜å…ˆ | `<HTML_OUTPUT>` æ ‡è®° > `<!DOCTYPE html>` > æ¨¡ç³ŠåŒ¹é… |
| ç‰ˆæœ¬å†å² | DB ç‰ˆæœ¬ä¸ºå‡† + æ–‡ä»¶é•œåƒ | DB ç‰ˆæœ¬ç®¡ç†ä¸ºä¸»ï¼Œæ–‡ä»¶ `v{timestamp}_*.html` ä½œä¸ºå¯é€‰é•œåƒ |
| Tool å¤±è´¥å¤„ç† | è®© LLM å†³å®š | ç»Ÿä¸€è¿”å›ç»“æ„åŒ– `{success, output, error}` ä¾› LLM å†³ç­– |

### è¯¦ç»†é—®ç­”

**Q: æµå¼ HTML è¾“å‡ºæ˜¯å¦éœ€è¦å®æ—¶æ˜¾ç¤ºç»™ç”¨æˆ·ï¼Ÿ**
> A: åç«¯ä»æµå¼å‘å°„äº‹ä»¶ï¼Œä½†å‰ç«¯é»˜è®¤åªåœ¨é˜¶æ®µå®Œæˆæ—¶æ˜¾ç¤ºç»“æœï¼›éœ€è¦æ—¶å¯ä»¥å¼€å¯å®æ—¶å±•ç¤ºã€‚

**Q: Token æˆæœ¬æ˜¯å¦åº”è¯¥å®æ—¶æ˜¾ç¤ºï¼Ÿ**
> A: ä¼šåœ¨è¯ç»“æŸæ—¶æ±‡æ€»æ˜¾ç¤ºã€‚å®æ—¶æ˜¾ç¤ºå¯èƒ½å¼•å‘ç„¦è™‘ï¼Œé™é»˜è®°å½•æ›´åˆé€‚ã€‚

**Q: æ¶ˆæ¯å†å²è¶…å‡ºä¸Šä¸‹æ–‡é™åˆ¶æ€ä¹ˆåŠï¼Ÿ**
> A: ä»¥ LLM ç®¡ç†ä¸ºä¸»ï¼ŒåŒæ—¶é¢„ç•™å¯æ’æ‹”æ‘˜è¦/è£å‰ªç­–ç•¥ç”¨äºè¶…é•¿ä¼šè¯ã€‚

**Q: HTML æå–å¤±è´¥æ€ä¹ˆåŠï¼Ÿ**
> A: é‡‡ç”¨ä¸‰çº§ç­–ç•¥ï¼š
> 1. ç‰¹æ®Šæ ‡è®° `<HTML_OUTPUT>...</HTML_OUTPUT>`ï¼ˆæœ€å¯é ï¼‰
> 2. `<!DOCTYPE html>...</html>` æ ‡å‡†æ ‡è®°
> 3. `<html>...</html>` æ¨¡ç³ŠåŒ¹é…

**Q: ç‰ˆæœ¬å†å²å¦‚ä½•ç®¡ç†ï¼Ÿ**
> A: DB ç‰ˆæœ¬ä¸ºå‡†ï¼Œæ–‡ä»¶ä½œä¸ºé•œåƒå¤‡ä»½ï¼š
> - DB: Version è¡¨ç»´æŠ¤å½“å‰/å†å²ç‰ˆæœ¬
> - æ–‡ä»¶: `index.html` å½“å‰é¢„è§ˆ + `v{timestamp}_*.html` é•œåƒ

**Q: Tool æ‰§è¡Œå¤±è´¥å¦‚ä½•å¤„ç†ï¼Ÿ**
> A: å§‹ç»ˆè¿”å›ç»“æ„åŒ– `{success, output, error}` ç»™ LLMï¼Œè®©å®ƒå†³å®šæ˜¯å¦é‡è¯•æˆ–æ¢æ–¹å¼ã€‚

---

## ç›®å½•

1. [ç‰ˆæœ¬æ¦‚è¿°](#1-ç‰ˆæœ¬æ¦‚è¿°)
2. [è®¾è®¡å†³ç­–è®¿è°ˆè®°å½•](#è®¾è®¡å†³ç­–è®¿è°ˆè®°å½•-2025-01-31)
3. [æ¶æ„è®¾è®¡](#3-æ¶æ„è®¾è®¡)
4. [LLM å®¢æˆ·ç«¯å®ç°](#4-llm-å®¢æˆ·ç«¯å®ç°)
5. [Agent ç³»ç»Ÿå®ç°](#5-agent-ç³»ç»Ÿå®ç°)
6. [Tools ç³»ç»Ÿå®ç°](#6-tools-ç³»ç»Ÿå®ç°)
7. [äº‹ä»¶é›†æˆ](#7-äº‹ä»¶é›†æˆ)
8. [Token è¿½è¸ª](#8-token-è¿½è¸ª)
9. [é”™è¯¯å¤„ç†](#9-é”™è¯¯å¤„ç†)
10. [æ–‡ä»¶å˜æ›´æ¸…å•](#10-æ–‡ä»¶å˜æ›´æ¸…å•)
11. [éªŒæ”¶æ ‡å‡†](#11-éªŒæ”¶æ ‡å‡†)

---

## 1. ç‰ˆæœ¬æ¦‚è¿°

### 1.1 ç‰ˆæœ¬å®šä½

**Spec v0.3** åœ¨ v0.2 (Web å‰ç«¯ + Planner) åŸºç¡€ä¸Šï¼Œå®ç° **Agent LLM è°ƒç”¨** å’Œ **Tools ç³»ç»Ÿ**ï¼Œå°†å ä½çš„ Agent æ›¿æ¢ä¸ºçœŸå®çš„ AI è°ƒç”¨ã€‚

**æ ¸å¿ƒå‡çº§**:
- ğŸ¤– **çœŸå® LLM è°ƒç”¨** - Interview/Generation/Refinement Agent çœŸæ­£è°ƒç”¨ AI
- ğŸ”§ **Tools ç³»ç»Ÿ** - æ”¯æŒ Function Callingï¼Œè®© AI èƒ½å¤Ÿæ‰§è¡Œå®é™…æ“ä½œ
- ğŸ“Š **Token è¿½è¸ª** - å®Œæ•´è®°å½•æ¯æ¬¡ LLM è°ƒç”¨çš„ Token æ¶ˆè€—
- ğŸ”„ **æµå¼è¾“å‡º** - åç«¯æµå¼äº‹ä»¶ï¼Œå‰ç«¯å¯æŒ‰éœ€å®æ—¶å±•ç¤º

### 1.2 ä¸ v0.2 çš„å…³ç³»

| v0.2 (å·²å®Œæˆ) | v0.3 (æœ¬ç‰ˆæœ¬) |
|--------------|--------------|
| Agent æ¶æ„ (ç©ºå£³) | Agent çœŸå® LLM è°ƒç”¨ |
| äº‹ä»¶ç³»ç»Ÿ (é™æ€) | äº‹ä»¶ç³»ç»Ÿ (å« Tool äº‹ä»¶) |
| å ä½è¿”å› | çœŸå® AI ç”Ÿæˆå†…å®¹ |
| æ—  Tool è°ƒç”¨ | æ”¯æŒ Function Calling |

### 1.3 æ ¸å¿ƒè®¾è®¡åŸåˆ™

**1.3.1 ç»Ÿä¸€çš„ LLM è°ƒç”¨å…¥å£**
```
æ‰€æœ‰ Agent é€šè¿‡ BaseAgent._call_llm() å‘èµ·è°ƒç”¨
    â†“
ç»Ÿä¸€çš„é”™è¯¯å¤„ç†å’Œé‡è¯•
    â†“
ç»Ÿä¸€çš„ Token è¿½è¸ª
    â†“
ç»Ÿä¸€çš„äº‹ä»¶å‘å°„
```

**1.3.2 Tool è°ƒç”¨é€æ˜åŒ–**
```
LLM è¿”å› tool_calls
    â†“
Agent æ‰§è¡Œ Tool (emit tool_call event)
    â†“
Tool è¿”å›ç»“æœ (emit tool_result event)
    â†“
ç»“æœè¿”å›ç»™ LLM ç»§ç»­å¤„ç†
```

**1.3.3 æµå¼å“åº”ä¼˜å…ˆ**
```
LLM æµå¼è¾“å‡º
    â†“
å®æ—¶å‘å°„ agent_progress äº‹ä»¶
    â†“
å‰ç«¯é»˜è®¤é˜¶æ®µæ€§å±•ç¤ºï¼ˆå¯åˆ‡æ¢å®æ—¶ï¼‰
    â†“
å®Œæ•´å“åº”åå‘å°„ agent_end äº‹ä»¶
```

---

## 3. æ¶æ„è®¾è®¡

### 3.1 æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Instant Coffee                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                      API Layer                             â”‚  â”‚
â”‚  â”‚  /api/chat â†’ Orchestrator â†’ Agents                        â”‚  â”‚
â”‚  â”‚  /api/plan â†’ Planner â†’ Executor (æŒ‰éœ€)                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â”‚                                  â”‚
â”‚                              â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                     Agent Layer                            â”‚  â”‚
â”‚  â”‚                                                         â”‚   â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚  â”‚
â”‚  â”‚  â”‚ Interview   â”‚  â”‚ Generation  â”‚  â”‚ Refinement      â”‚  â”‚   â”‚  â”‚
â”‚  â”‚  â”‚   Agent     â”‚  â”‚   Agent     â”‚  â”‚   Agent         â”‚  â”‚   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚  â”‚
â”‚  â”‚         â”‚                â”‚                  â”‚           â”‚   â”‚  â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚   â”‚  â”‚
â”‚  â”‚                          â–¼                              â”‚   â”‚  â”‚
â”‚  â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚   â”‚  â”‚
â”‚  â”‚                   â”‚  BaseAgent  â”‚                       â”‚   â”‚  â”‚
â”‚  â”‚                   â”‚  +_call_llm â”‚                       â”‚   â”‚  â”‚
â”‚  â”‚                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                       â”‚   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚                             â”‚                                     â”‚
â”‚                             â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    LLM Layer                              â”‚  â”‚
â”‚  â”‚                                                         â”‚   â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚   â”‚  â”‚
â”‚  â”‚  â”‚              OpenAIClient                           â”‚â”‚   â”‚  â”‚
â”‚  â”‚  â”‚  - chat_completion()    - éæµå¼è°ƒç”¨               â”‚â”‚   â”‚  â”‚
â”‚  â”‚  â”‚  - chat_completion_stream()  - æµå¼è°ƒç”¨            â”‚â”‚   â”‚  â”‚
â”‚  â”‚  â”‚  - chat_with_tools()    - Tool è°ƒç”¨                â”‚â”‚   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚   â”‚  â”‚
â”‚  â”‚                                                         â”‚   â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚   â”‚  â”‚
â”‚  â”‚  â”‚              Tools Registry                         â”‚â”‚   â”‚  â”‚
â”‚  â”‚  â”‚  - filesystem_write / filesystem_read               â”‚â”‚   â”‚  â”‚
â”‚  â”‚  â”‚  - validate_html                                    â”‚â”‚   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â”‚                                  â”‚
â”‚                              â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                   Services Layer                          â”‚  â”‚
â”‚  â”‚  - TokenTrackerService  - Token è¿½è¸ª                     â”‚  â”‚
â”‚  â”‚  - EventEmitter        - äº‹ä»¶å‘å°„                        â”‚  â”‚
â”‚  â”‚  - FilesystemService   - æ–‡ä»¶æ“ä½œ                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 æ–‡ä»¶ç»“æ„

```
packages/backend/app/
â”œâ”€â”€ llm/                          # æ–°å¢: LLM è°ƒç”¨å±‚
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ openai_client.py          # OpenAI SDK å°è£…
â”‚   â””â”€â”€ tools.py                  # Tools å®šä¹‰
â”‚
â”œâ”€â”€ agents/                       # ç°æœ‰: Agent å±‚
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                   # ä¿®æ”¹: æ·»åŠ  _call_llm
â”‚   â”œâ”€â”€ prompts.py                # æ–°å¢: Agent System Prompts
â”‚   â”œâ”€â”€ interview.py              # ä¿®æ”¹: å®ç°çœŸå® LLM è°ƒç”¨
â”‚   â”œâ”€â”€ generation.py             # ä¿®æ”¹: å®ç°çœŸå® LLM è°ƒç”¨
â”‚   â””â”€â”€ refinement.py             # ä¿®æ”¹: å®ç°çœŸå® LLM è°ƒç”¨
â”‚
â”œâ”€â”€ events/                       # ç°æœ‰: äº‹ä»¶ç³»ç»Ÿ
â”‚   â”œâ”€â”€ models.py                 # å·²å®Œæˆ
â”‚   â”œâ”€â”€ emitter.py                # å·²å®Œæˆ
â”‚   â””â”€â”€ types.py                  # å·²å®Œæˆ
â”‚
â”œâ”€â”€ services/                     # ç°æœ‰: æœåŠ¡å±‚
â”‚   â”œâ”€â”€ token_tracker.py          # å·²å®Œæˆ
â”‚   â””â”€â”€ filesystem.py             # å·²å®Œæˆ
â”‚
â””â”€â”€ config.py                     # ç°æœ‰: é…ç½® (å·²æ”¯æŒ OpenAI)
```

### 3.3 æ•°æ®æµ

**æ™®é€šå¯¹è¯æµç¨‹**:
```
ç”¨æˆ·è¾“å…¥
    â†“
InterviewAgent.process()
    â†“
BaseAgent._call_llm(messages)
    â†“
å‘å°„ agent_start äº‹ä»¶
    â†“
OpenAIClient.chat_completion() / chat_completion_stream()
    â†“
ï¼ˆæµå¼ï¼‰æŒç»­å‘å°„ agent_progress
    â†“
TokenTrackerService.record_usage()
    â†“
å‘å°„ agent_end äº‹ä»¶
    â†“
è¿”å› AgentResult
```
> æ³¨: æµå¼è°ƒç”¨å¯èƒ½æ— æ³•ç›´æ¥è·å¾— usageï¼›éœ€è¦æ—¶å¯å¯ç”¨ `stream_options.include_usage` æˆ–æ”¹ç”¨éæµå¼è°ƒç”¨ç»Ÿè®¡ã€‚

**Tool è°ƒç”¨æµç¨‹**:
```
LLM è¿”å› tool_calls
    â†“
Agent éå† tool_calls
    â†“
å‘å°„ tool_call äº‹ä»¶
    â†“
æ‰§è¡Œ Tool (è°ƒç”¨å¯¹åº”çš„ Tool Handler)
    â†“
å‘å°„ tool_result äº‹ä»¶
    â†“
æ”¶é›†æ‰€æœ‰ Tool ç»“æœ
    â†“
å°†ç»“æœè¿”å›ç»™ LLM (ä½œä¸º assistant æ¶ˆæ¯ + tool_results)
    â†“
ç»§ç»­ç­‰å¾… LLM å“åº”
    â†“
TokenTrackerService.record_usage() (ç´¯è®¡æ‰€æœ‰ Token)
```

---

## 4. LLM å®¢æˆ·ç«¯å®ç°

### 4.1 OpenAIClient ç±»è®¾è®¡

**æ–‡ä»¶**: `packages/backend/app/llm/openai_client.py`

```python
from __future__ import annotations

import json
import logging
from dataclasses import dataclass
from typing import AsyncGenerator, AsyncIterator, Dict, List, Optional, Union

from openai import AsyncOpenAI
from openai.types import ChatCompletion, ChatCompletionChunk, ChatCompletionMessageToolCall
from openai.types.chat import ChatCompletionToolMessageParam

from ...config import Settings, get_settings
from ...services.token_tracker import TokenTrackerService
from .tools import Tool, ToolResult

logger = logging.getLogger(__name__)


@dataclass
class TokenUsage:
    """Token ä½¿ç”¨é‡ç»Ÿè®¡"""
    input_tokens: int
    output_tokens: int
    total_tokens: int
    cost_usd: float


@dataclass
class LLMResponse:
    """LLM å“åº”"""
    content: str
    tool_calls: Optional[List[ChatCompletionMessageToolCall]] = None
    token_usage: Optional[TokenUsage] = None


class OpenAIClientError(Exception):
    """OpenAI å®¢æˆ·ç«¯é”™è¯¯"""
    pass


class RateLimitError(OpenAIClientError):
    """é€Ÿç‡é™åˆ¶é”™è¯¯"""
    pass


class APIError(OpenAIClientError):
    """API é”™è¯¯"""
    pass


class OpenAIClient:
    """
    OpenAI API å®¢æˆ·ç«¯å°è£…

    æä¾›:
    - éæµå¼å¯¹è¯
    - æµå¼å¯¹è¯
    - Tool/Function Calling
    - Token ä½¿ç”¨é‡è¿½è¸ª
    - ç»Ÿä¸€çš„é”™è¯¯å¤„ç†
    """

    def __init__(
        self,
        settings: Optional[Settings] = None,
        token_tracker: Optional[TokenTrackerService] = None,
    ) -> None:
        self.settings = settings or get_settings()
        self.token_tracker = token_tracker

        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        self.client = AsyncOpenAI(
            api_key=self.settings.openai_api_key,
            base_url=self.settings.openai_base_url,
        )

        # æ¨¡å‹å®šä»· (USD per 1M tokens)
        self._pricing = {
            "gpt-4o": {"input": 5.0, "output": 15.0},
            "gpt-4o-mini": {"input": 0.15, "output": 0.60},
            "gpt-4-turbo": {"input": 10.0, "output": 30.0},
            "gpt-3.5-turbo": {"input": 0.50, "output": 1.50},
        }

    def _get_pricing(self, model: str) -> Dict[str, float]:
        """è·å–æ¨¡å‹å®šä»·"""
        # å°è¯•ç²¾ç¡®åŒ¹é…
        if model in self._pricing:
            return self._pricing[model]
        # æ¨¡ç³ŠåŒ¹é… (å–å‰ç¼€)
        for prefix, pricing in self._pricing.items():
            if model.startswith(prefix):
                return pricing
        # é»˜è®¤å®šä»·
        return {"input": 1.0, "output": 3.0}

    def _calculate_cost(self, model: str, input_tokens: int, output_tokens: int) -> float:
        """è®¡ç®— API è°ƒç”¨æˆæœ¬ (USD)"""
        pricing = self._get_pricing(model)
        input_cost = (input_tokens / 1_000_000) * pricing["input"]
        output_cost = (output_tokens / 1_000_000) * pricing["output"]
        return input_cost + output_cost

    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        tools: Optional[List[Tool]] = None,
        **kwargs,
    ) -> LLMResponse:
        """
        éæµå¼å¯¹è¯å®Œæˆ

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§° (é»˜è®¤ä½¿ç”¨ settings ä¸­çš„é…ç½®)
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§è¾“å‡º token
            tools: å¯ç”¨çš„å·¥å…·åˆ—è¡¨

        Returns:
            LLMResponse å¯¹è±¡
        """
        model = model or self.settings.model
        temperature = temperature if temperature is not None else self.settings.temperature
        max_tokens = max_tokens or self.settings.max_tokens

        try:
            # æ„å»ºè¯·æ±‚å‚æ•°
            params = {
                "model": model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                **kwargs,
            }

            if tools:
                params["tools"] = [tool.to_openai_format() for tool in tools]
                params["tool_choice"] = "auto"

            # å‘é€è¯·æ±‚
            response: ChatCompletion = await self.client.chat.completions.create(**params)

            # è§£æå“åº”
            choice = response.choices[0]
            message = choice.message

            content = message.content or ""
            tool_calls = message.tool_calls

            # è®¡ç®— Token ä½¿ç”¨é‡
            usage = response.usage
            token_usage = TokenUsage(
                input_tokens=usage.prompt_tokens,
                output_tokens=usage.completion_tokens,
                total_tokens=usage.total_tokens,
                cost_usd=self._calculate_cost(
                    model,
                    usage.prompt_tokens,
                    usage.completion_tokens
                ),
            )

            return LLMResponse(
                content=content,
                tool_calls=tool_calls,
                token_usage=token_usage,
            )

        except Exception as e:
            logger.exception("LLM API è°ƒç”¨å¤±è´¥")
            raise self._handle_error(e)

    async def chat_completion_stream(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        stream_options: Optional[dict] = None,
        **kwargs,
    ) -> AsyncIterator[str]:
        """
        æµå¼å¯¹è¯å®Œæˆ

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§è¾“å‡º token

        Yields:
            æµå¼è¾“å‡ºçš„æ–‡æœ¬ç‰‡æ®µ
        """
        model = model or self.settings.model
        temperature = temperature if temperature is not None else self.settings.temperature
        max_tokens = max_tokens or self.settings.max_tokens

        try:
            params = {
                "model": model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": True,
                **kwargs,
            }
            if stream_options:
                params["stream_options"] = stream_options

            stream = await self.client.chat.completions.create(**params)

            async for chunk in stream:
                if chunk.choices[0].delta.content is not None:
                    yield chunk.choices[0].delta.content

        except Exception as e:
            logger.exception("LLM æµå¼è°ƒç”¨å¤±è´¥")
            raise self._handle_error(e)

    async def chat_with_tools(
        self,
        messages: List[Dict[str, str]],
        tools: List[Tool],
        tool_handlers: Dict[str, callable],
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_iterations: int = 10,
        **kwargs,
    ) -> LLMResponse:
        """
        å¸¦ Tool è°ƒç”¨çš„å¯¹è¯

        å¤„ç†å®Œæ•´çš„ tool calling å¾ªç¯:
        1. å‘é€è¯·æ±‚
        2. å¦‚æœ LLM è¿”å› tool_callsï¼Œæ‰§è¡Œå¹¶æ”¶é›†ç»“æœ
        3. å°†ç»“æœè¿”å›ç»™ LLM
        4. é‡å¤ç›´åˆ° LLM ä¸å†éœ€è¦è°ƒç”¨å·¥å…·

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ (ä¼šè¢«ä¿®æ”¹ï¼ŒåŒ…å« tool ç»“æœ)
            tools: å¯ç”¨çš„å·¥å…·
            tool_handlers: å·¥å…·å -> å¤„ç†å‡½æ•° çš„æ˜ å°„
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•° (é˜²æ­¢æ— é™å¾ªç¯)

        Returns:
            æœ€ç»ˆçš„ LLMResponse
        """
        model = model or self.settings.model
        temperature = temperature if temperature is not None else self.settings.temperature

        total_input_tokens = 0
        total_output_tokens = 0

        for iteration in range(max_iterations):
            # å‘é€è¯·æ±‚
            response = await self.chat_completion(
                messages=messages,
                model=model,
                temperature=temperature,
                tools=tools,
                **kwargs,
            )

            # ç´¯è®¡ Token
            if response.token_usage:
                total_input_tokens += response.token_usage.input_tokens
                total_output_tokens += response.token_usage.output_tokens

            # å¦‚æœæ²¡æœ‰ tool_callsï¼Œå®Œæˆ
            if not response.tool_calls:
                break

            # æ‰§è¡Œ tool calls
            tool_messages = []
            for tool_call in response.tool_calls:
                tool_name = tool_call.function.name
                tool_id = tool_call.id
                arguments = tool_call.function.arguments

                # è°ƒç”¨å·¥å…·å¤„ç†å‡½æ•°
                try:
                    handler = tool_handlers.get(tool_name)
                    if handler:
                        result = await handler(**json.loads(arguments))
                        success = True
                        output = result
                    else:
                        success = False
                        output = None
                        error = f"Unknown tool: {tool_name}"
                except Exception as e:
                    success = False
                    output = None
                    error = str(e)

                # æ„å»º tool ç»“æœæ¶ˆæ¯ (å§‹ç»ˆä½¿ç”¨ç»“æ„åŒ– JSON)
                tool_messages.append({
                    "role": "tool",
                    "tool_call_id": tool_id,
                    "content": json.dumps({
                        "success": success,
                        "output": output,
                        "error": error,
                    }, ensure_ascii=False),
                })

            # å°† tool ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²
            messages.append({
                "role": "assistant",
                "content": response.content,
                "tool_calls": response.tool_calls,
            })
            messages.extend(tool_messages)

        else:
            logger.warning(f"Tool è°ƒç”¨è¶…è¿‡ {max_iterations} æ¬¡ï¼Œå¼ºåˆ¶é€€å‡º")

        # è¿”å›æœ€ç»ˆç»“æœ
        if total_input_tokens > 0 or total_output_tokens > 0:
            cost = self._calculate_cost(model, total_input_tokens, total_output_tokens)
            token_usage = TokenUsage(
                input_tokens=total_input_tokens,
                output_tokens=total_output_tokens,
                total_tokens=total_input_tokens + total_output_tokens,
                cost_usd=cost,
            )
            return LLMResponse(
                content=response.content,
                token_usage=token_usage,
            )

        return response

    def _handle_error(self, error: Exception) -> OpenAIClientError:
        """ç»Ÿä¸€é”™è¯¯å¤„ç†"""
        error_str = str(error).lower()

        if "rate_limit" in error_str or "429" in error_str:
            return RateLimitError(f"é€Ÿç‡é™åˆ¶: {error}")

        if "invalid_api_key" in error_str or "401" in error_str:
            return APIError(f"æ— æ•ˆçš„ API Key: {error}")

        if "timeout" in error_str or "504" in error_str:
            return APIError(f"è¯·æ±‚è¶…æ—¶: {error}")

        return APIError(f"LLM API é”™è¯¯: {error}")


__all__ = [
    "OpenAIClient",
    "OpenAIClientError",
    "RateLimitError",
    "APIError",
    "LLMResponse",
    "TokenUsage",
]
```

> æ³¨: æµå¼è°ƒç”¨è‹¥éœ€ usageï¼Œå¯åœ¨æ”¯æŒçš„å¹³å°ä¼  `stream_options={"include_usage": true}` å¹¶è‡ªè¡Œç´¯è®¡ï¼›å¦åˆ™ä½¿ç”¨éæµå¼ç»Ÿè®¡ã€‚

### 4.2 Tools å®šä¹‰

**æ–‡ä»¶**: `packages/backend/app/llm/tools.py`

```python
from __future__ import annotations

import json
from dataclasses import dataclass
from typing import Any, Dict, List, Optional

import pydantic


class BaseModel(pydantic.BaseModel):
    """Pydantic åŸºç¡€æ¨¡å‹"""
    pass


# ============ Tool Schema å®šä¹‰ ============

@dataclass
class WriteFileParams:
    """å†™å…¥æ–‡ä»¶å‚æ•°"""
    path: str
    content: str
    encoding: str = "utf-8"


@dataclass
class ReadFileParams:
    """è¯»å–æ–‡ä»¶å‚æ•°"""
    path: str
    encoding: str = "utf-8"


@dataclass
class ValidateHtmlParams:
    """HTML éªŒè¯å‚æ•°"""
    html: str


@dataclass
class ListDirParams:
    """åˆ—å‡ºç›®å½•å†…å®¹å‚æ•°"""
    path: str
    pattern: Optional[str] = None


@dataclass
class ExistsParams:
    """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å‚æ•°"""
    path: str


# ============ Tool ç±» ============

class Tool:
    """
    Tool å®šä¹‰

    ç”¨äºæè¿° AI å¯ä»¥è°ƒç”¨çš„å·¥å…·ã€‚
    """

    def __init__(
        self,
        name: str,
        description: str,
        parameters: Dict[str, Any],
    ) -> None:
        self.name = name
        self.description = description
        self.parameters = parameters

    def to_openai_format(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸º OpenAI Tool Format"""
        return {
            "type": "function",
            "function": {
                "name": self.name,
                "description": self.description,
                "parameters": self.parameters,
            },
        }


class ToolResult:
    """
    Tool æ‰§è¡Œç»“æœ
    """

    def __init__(
        self,
        success: bool,
        output: Any = None,
        error: Optional[str] = None,
    ) -> None:
        self.success = success
        self.output = output
        self.error = error

    def to_dict(self) -> Dict[str, Any]:
        result = {
            "success": self.success,
        }
        if self.output is not None:
            result["output"] = self.output
        if self.error is not None:
            result["error"] = self.error
        return result


# ============ é¢„å®šä¹‰ Tools ============

# Filesystem Write Tool
FILESYSTEM_WRITE_TOOL = Tool(
    name="filesystem_write",
    description="Write content to a file. Use this to save generated HTML or other files.",
    parameters={
        "type": "object",
        "properties": {
            "path": {
                "type": "string",
                "description": "The file path to write to",
            },
            "content": {
                "type": "string",
                "description": "The content to write",
            },
            "encoding": {
                "type": "string",
                "description": "File encoding (default: utf-8)",
                "enum": ["utf-8", "gbk"],
                "default": "utf-8",
            },
        },
        "required": ["path", "content"],
        "additionalProperties": False,
    },
)

# Filesystem Read Tool
FILESYSTEM_READ_TOOL = Tool(
    name="filesystem_read",
    description="Read content from a file. Use this to read existing HTML files.",
    parameters={
        "type": "object",
        "properties": {
            "path": {
                "type": "string",
                "description": "The file path to read from",
            },
            "encoding": {
                "type": "string",
                "description": "File encoding (default: utf-8)",
                "enum": ["utf-8", "gbk"],
                "default": "utf-8",
            },
        },
        "required": ["path"],
        "additionalProperties": False,
    },
)

# Validate HTML Tool
VALIDATE_HTML_TOOL = Tool(
    name="validate_html",
    description="Validate HTML content against mobile-first standards.",
    parameters={
        "type": "object",
        "properties": {
            "html": {
                "type": "string",
                "description": "The HTML content to validate",
            },
        },
        "required": ["html"],
        "additionalProperties": False,
    },
)


def get_filesystem_tools() -> List[Tool]:
    """è·å–æ–‡ä»¶ç³»ç»Ÿç›¸å…³ tools"""
    return [FILESYSTEM_WRITE_TOOL, FILESYSTEM_READ_TOOL]


def get_all_tools() -> List[Tool]:
    """è·å–æ‰€æœ‰å¯ç”¨çš„ tools"""
    return [
        FILESYSTEM_WRITE_TOOL,
        FILESYSTEM_READ_TOOL,
        VALIDATE_HTML_TOOL,
    ]

# è¯´æ˜: ListDirParams / ExistsParams ä¸ºé¢„ç•™å·¥å…·å‚æ•°ï¼Œæš‚ä¸æš´éœ²ç»™ LLMã€‚


__all__ = [
    "Tool",
    "ToolResult",
    "WriteFileParams",
    "ReadFileParams",
    "ValidateHtmlParams",
    "ListDirParams",
    "ExistsParams",
    "FILESYSTEM_WRITE_TOOL",
    "FILESYSTEM_READ_TOOL",
    "VALIDATE_HTML_TOOL",
    "get_filesystem_tools",
    "get_all_tools",
]
```

---

## 5. Agent ç³»ç»Ÿå®ç°

### 5.1 BaseAgent ä¿®æ”¹

**æ–‡ä»¶**: `packages/backend/app/agents/base.py`

```python
from __future__ import annotations

from dataclasses import dataclass, field
from typing import List, Optional, Sequence

from ..config import Settings
from ..events.emitter import EventEmitter
from ..events.models import (
    AgentEndEvent,
    AgentProgressEvent,
    AgentStartEvent,
    ToolCallEvent,
    ToolResultEvent,
)
from ..llm.openai_client import OpenAIClient, LLMResponse
from ..services.token_tracker import TokenTrackerService


class APIError(Exception):
    """Generic API error."""


class RateLimitError(APIError):
    """Rate limit exceeded."""


@dataclass
class AgentResult:
    message: str
    is_complete: bool = True
    confidence: Optional[float] = None
    context: Optional[str] = None
    rounds_used: Optional[int] = None
    token_usage: Optional[dict] = None


class BaseAgent:
    """
    Agent åŸºç±»

    æä¾›:
    - LLM å®¢æˆ·ç«¯åˆå§‹åŒ–
    - ç»Ÿä¸€çš„ _call_llm() æ–¹æ³•
    - äº‹ä»¶å‘å°„æ–¹æ³•
    - Tool è°ƒç”¨æ”¯æŒ
    """

    agent_type: str = "base"

    def __init__(
        self,
        db,
        session_id: str,
        settings: Settings,
        event_emitter: Optional[EventEmitter] = None,
        agent_id: Optional[str] = None,
        task_id: Optional[str] = None,
        token_tracker: Optional[TokenTrackerService] = None,
    ) -> None:
        self.db = db
        self.session_id = session_id
        self.settings = settings
        self.event_emitter = event_emitter
        self.agent_id = agent_id or f"{self.agent_type}_1"
        self.task_id = task_id
        self.token_tracker = token_tracker

        # åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
        self._llm_client = OpenAIClient(
            settings=settings,
            token_tracker=token_tracker,
        )

    async def _call_llm(
        self,
        messages: List[dict],
        *,
        agent_type: Optional[str] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        tools: Optional[List] = None,
        stream: bool = False,
        emit_progress: bool = True,
        context: Optional[str] = None,
    ) -> LLMResponse:
        """
        ç»Ÿä¸€çš„ LLM è°ƒç”¨å…¥å£

        Args:
            messages: æ¶ˆæ¯å†å²
            agent_type: Agent ç±»å‹ (ç”¨äº Token è¿½è¸ª)
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦
            max_tokens: æœ€å¤§ token
            tools: å¯ç”¨çš„å·¥å…·åˆ—è¡¨
            stream: æ˜¯å¦æµå¼è¾“å‡º
            emit_progress: æ˜¯å¦å‘å°„è¿›åº¦äº‹ä»¶
            context: é¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯

        Returns:
            LLMResponse å¯¹è±¡
        """
        agent_type = agent_type or self.agent_type

        # å‘å°„å¼€å§‹äº‹ä»¶
        self._emit_agent_start(context=context)

        try:
            if stream:
                # æµå¼è°ƒç”¨
                full_response = ""
                async for chunk in self._llm_client.chat_completion_stream(
                    messages=messages,
                    model=model,
                    temperature=temperature,
                    max_tokens=max_tokens,
                ):
                    full_response += chunk
                    if emit_progress and chunk:
                        progress = min(90, 10 + len(full_response) // 100)
                        self._emit_agent_progress(
                            message=chunk[-200:],
                            progress=progress,
                        )
                response = LLMResponse(content=full_response)
            else:
                # éæµå¼è°ƒç”¨
                response = await self._llm_client.chat_completion(
                    messages=messages,
                    model=model,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    tools=tools,
                )

            # è®°å½• Token ä½¿ç”¨
            if response.token_usage and self.token_tracker:
                self.token_tracker.record_usage(
                    session_id=self.session_id,
                    agent_type=agent_type,
                    model=model or self.settings.model,
                    input_tokens=response.token_usage.input_tokens,
                    output_tokens=response.token_usage.output_tokens,
                    cost_usd=response.token_usage.cost_usd,
                )

            # å‘å°„ç»“æŸäº‹ä»¶
            self._emit_agent_end(
                status="success",
                summary=response.content[:200] if response.content else None,
            )

            return response

        except Exception as e:
            # å‘å°„å¤±è´¥äº‹ä»¶
            self._emit_agent_end(
                status="failed",
                summary=str(e),
            )
            raise

    async def _call_llm_with_tools(
        self,
        messages: List[dict],
        tools: List,
        tool_handlers: dict,
        *,
        agent_type: Optional[str] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_iterations: int = 10,
        context: Optional[str] = None,
    ) -> LLMResponse:
        """
        å¸¦æœ‰ Tool è°ƒç”¨çš„ LLM è°ƒç”¨

        å¤„ç†å®Œæ•´çš„ tool calling å¾ªç¯ï¼Œå¹¶å‘å°„ tool_call/tool_result äº‹ä»¶ã€‚
        """
        agent_type = agent_type or self.agent_type

        # å‘å°„å¼€å§‹äº‹ä»¶
        self._emit_agent_start(context=context)

        # åŒ…è£… tool handlers ä»¥æ·»åŠ äº‹ä»¶å‘å°„
        wrapped_handlers = {}
        for tool_name, handler in tool_handlers.items():
            wrapped_handlers[tool_name] = self._wrap_tool_handler(
                tool_name,
                handler,
                agent_type,
            )

        try:
            response = await self._llm_client.chat_with_tools(
                messages=messages,
                tools=tools,
                tool_handlers=wrapped_handlers,
                model=model,
                temperature=temperature,
                max_iterations=max_iterations,
            )

            # è®°å½• Token ä½¿ç”¨
            if response.token_usage and self.token_tracker:
                self.token_tracker.record_usage(
                    session_id=self.session_id,
                    agent_type=agent_type,
                    model=model or self.settings.model,
                    input_tokens=response.token_usage.input_tokens,
                    output_tokens=response.token_usage.output_tokens,
                    cost_usd=response.token_usage.cost_usd,
                )

            # å‘å°„ç»“æŸäº‹ä»¶
            self._emit_agent_end(
                status="success",
                summary=response.content[:200] if response.content else None,
            )

            return response

        except Exception as e:
            self._emit_agent_end(
                status="failed",
                summary=str(e),
            )
            raise

    def _emit_agent_start(self, context: Optional[str] = None) -> None:
        """å‘å°„ agent_start äº‹ä»¶"""
        if self.event_emitter:
            event = AgentStartEvent(
                agent_id=self.agent_id,
                agent_type=self.agent_type,
                task_id=self.task_id,
            )
            self.event_emitter.emit(event)

    def _emit_agent_progress(self, message: str, progress: int) -> None:
        """å‘å°„ agent_progress äº‹ä»¶"""
        if self.event_emitter:
            event = AgentProgressEvent(
                agent_id=self.agent_id,
                agent_type=self.agent_type,
                task_id=self.task_id,
                message=message,
                progress=progress,
            )
            self.event_emitter.emit(event)

    def _emit_agent_end(self, status: str, summary: Optional[str] = None) -> None:
        """å‘å°„ agent_end äº‹ä»¶"""
        if self.event_emitter:
            event = AgentEndEvent(
                agent_id=self.agent_id,
                agent_type=self.agent_type,
                task_id=self.task_id,
                status=status,
                summary=summary,
            )
            self.event_emitter.emit(event)

    def _wrap_tool_handler(
        self,
        tool_name: str,
        handler: callable,
        agent_type: str,
    ) -> callable:
        """
        åŒ…è£… tool handler ä»¥æ·»åŠ äº‹ä»¶å‘å°„
        """
        async def wrapped_handler(**kwargs):
            # å‘å°„ tool_call äº‹ä»¶
            if self.event_emitter:
                self.event_emitter.emit(ToolCallEvent(
                    agent_id=self.agent_id,
                    agent_type=agent_type,
                    task_id=self.task_id,
                    tool_name=tool_name,
                    tool_input=kwargs,
                ))

            try:
                result = await handler(**kwargs)

                # å‘å°„ tool_result äº‹ä»¶ (æˆåŠŸ)
                if self.event_emitter:
                    self.event_emitter.emit(ToolResultEvent(
                        agent_id=self.agent_id,
                        agent_type=agent_type,
                        task_id=self.task_id,
                        tool_name=tool_name,
                        success=True,
                        tool_output=result,
                    ))

                return result

            except Exception as e:
                # å‘å°„ tool_result äº‹ä»¶ (å¤±è´¥)
                if self.event_emitter:
                    self.event_emitter.emit(ToolResultEvent(
                        agent_id=self.agent_id,
                        agent_type=agent_type,
                        task_id=self.task_id,
                        tool_name=tool_name,
                        success=False,
                        error=str(e),
                    ))
                raise

        return wrapped_handler


__all__ = ["APIError", "RateLimitError", "AgentResult", "BaseAgent"]
```

### 5.2 Agent Prompts

**æ–‡ä»¶**: `packages/backend/app/agents/prompts.py`

```python
"""
Agent System Prompts

åŒ…å«æ‰€æœ‰ Agent çš„ System Prompt æ¨¡æ¿ã€‚
"""

# ============ Interview Agent Prompt ============

INTERVIEW_SYSTEM_PROMPT = """ä½ æ˜¯ Instant Coffee çš„ Interview Agentï¼Œè´Ÿè´£é€šè¿‡å¯¹è¯äº†è§£ç”¨æˆ·éœ€æ±‚ã€‚

ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. åˆ†æç”¨æˆ·çš„è¾“å…¥ï¼Œåˆ¤æ–­ä¿¡æ¯å……åˆ†åº¦ (0-100%)
2. æ ¹æ®ä¿¡æ¯å……åˆ†åº¦å†³å®šæ˜¯å¦éœ€è¦ç»§ç»­æé—®:
   - 90%+ â†’ ä¿¡æ¯éå¸¸å……åˆ†ï¼Œå¯ä»¥ç›´æ¥ç”Ÿæˆ
   - 70-90% â†’ ä¿¡æ¯è¾ƒå……åˆ†ï¼Œé—® 1-2 ä¸ªå…³é”®é—®é¢˜
   - 50-70% â†’ ä¿¡æ¯ä¸€èˆ¬ï¼Œé—® 2-3 ä¸ªé—®é¢˜
   - <50% â†’ ä¿¡æ¯ä¸è¶³ï¼Œå¤šé—®å‡ ä¸ªé—®é¢˜äº†è§£éœ€æ±‚
3. æ¯è½®æœ€å¤šé—® 3 ä¸ªé—®é¢˜
4. é—®é¢˜è¦å…·ä½“ã€æ˜“äºå›ç­”
5. æä¾›é€‰é¡¹è®©ç”¨æˆ·é€‰æ‹©ï¼ŒåŒæ—¶æ”¯æŒæ–‡å­—è¾“å…¥

è¾“å‡ºæ ¼å¼ (JSON):
{{
  "message": "å±•ç¤ºç»™ç”¨æˆ·çš„é—®é¢˜æ–‡æœ¬ (æ”¯æŒ Markdown)",
  "is_complete": true/false,
  "confidence": 0.0-1.0,
  "collected_info": {{"å·²æ”¶é›†çš„ä¿¡æ¯": "å€¼"}},
  "missing_info": ["è¿˜ç¼ºå°‘çš„ä¿¡æ¯"]
}}

æ³¨æ„äº‹é¡¹:
- ä½¿ç”¨å‹å¥½ã€æ—¥å¸¸çš„è¯­è¨€
- ç”¨ Emoji è®©å¯¹è¯æ›´æœ‰äº²å’ŒåŠ› âœ¨
- ä¸ä½¿ç”¨æŠ€æœ¯æœ¯è¯­
- å¦‚æœç”¨æˆ·å·²ç»æä¾›äº†è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œæœæ–­ç»“æŸæé—®
- æ”¶é›†çš„ä¿¡æ¯è¦ç»“æ„åŒ–ï¼Œæ–¹ä¾¿åç»­ç”Ÿæˆ
"""


# ============ Generation Agent Prompt ============

GENERATION_SYSTEM_PROMPT = """ä½ æ˜¯ Instant Coffee çš„ Generation Agentï¼Œè´Ÿè´£ç”Ÿæˆç§»åŠ¨ç«¯ä¼˜åŒ–çš„ HTML é¡µé¢ã€‚

## ç§»åŠ¨ç«¯è®¾è®¡è¦æ±‚ (å¿…é¡»éµå®ˆ)

### è§†å£å’Œå®¹å™¨
- è§†å£æ¯”ä¾‹: 9:19.5 (æ‰‹æœºç«–å±)
- æœ€å¤§å®½åº¦: 430px (iPhone Pro Max)
- å±…ä¸­æ˜¾ç¤ºï¼Œä¸¤ä¾§ç•™ç™½

### åŸºç¡€æ ·å¼
- å­—ä½“: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif
- æ­£æ–‡å­—å·: 16px
- æ ‡é¢˜å­—å·: 24-32px
- è¡Œé«˜: 1.5-1.6

### äº¤äº’å…ƒç´ 
- æŒ‰é’®æœ€å°é«˜åº¦: 44px (è§¦æ‘¸ä¼˜åŒ–)
- è§¦æ‘¸åŒºåŸŸè¶³å¤Ÿå¤§
- ç¦ç”¨åŒå‡»ç¼©æ”¾

### æ»šåŠ¨æ¡å¤„ç†
- éšè—æ»šåŠ¨æ¡: ä½¿ç”¨ .hide-scrollbar ç±»
```css
.hide-scrollbar {{
    -ms-overflow-style: none;
    scrollbar-width: none;
}}
.hide-scrollbar::-webkit-scrollbar {{
    display: none;
}}
```

### é¢œè‰²ç³»ç»Ÿ
- ä½¿ç”¨æŸ”å’Œçš„æ¸å˜è‰²èƒŒæ™¯
- ä¸»è¦æ“ä½œæŒ‰é’®ä½¿ç”¨å“ç‰Œè‰² (#007AFF ç­‰)
- æ–‡å­—ä½¿ç”¨æ·±è‰² (#1a1a1a)
- èƒŒæ™¯ä½¿ç”¨æµ…è‰² (#f5f5f7)

## æŠ€æœ¯è¦æ±‚

1. å•æ–‡ä»¶ HTML (CSS å’Œ JS å†…è”)
2. æ— å¤–éƒ¨ä¾èµ– (é™¤ Google Fonts)
3. ä½¿ç”¨ç°ä»£ CSS (Flexbox/Grid)
4. å›¾ç‰‡å“åº”å¼ (max-width: 100%)
5. æ”¯æŒè§¦æ‘¸äº¤äº’

## è¾“å‡ºæ ¼å¼

ç›´æ¥è¾“å‡ºå®Œæ•´çš„ HTML ä»£ç ï¼Œç”¨ç‰¹æ®Šæ ‡è®°åŒ…è£¹ä»¥ä¾¿æå–:

```
<HTML_OUTPUT>
<!DOCTYPE html>
<html>
...
</html>
</HTML_OUTPUT>
```

ä¸è¦åœ¨ä»»ä½•å…¶ä»–å†…å®¹ï¼ˆå¦‚ä»£ç å—æ ‡è®° ```html æˆ–é¢å¤–è¯´æ˜ï¼‰ï¼Œåªè¾“å‡ºè¿™ä¸ªæ ‡è®°åŒ…è£¹çš„ HTMLã€‚

## æ¸è¿›å¼ç”Ÿæˆ

ç”Ÿæˆè¿‡ç¨‹åˆ†ä¸º 5 ä¸ªé˜¶æ®µ:
1. 20%: é¡µé¢ç»“æ„ (éª¨æ¶)
2. 40%: æ ¸å¿ƒå†…å®¹
3. 60%: æ ·å¼åº”ç”¨
4. 80%: äº¤äº’é€»è¾‘
5. 100%: ç§»åŠ¨ç«¯ä¼˜åŒ–

å¦‚æœéœ€è¦ä¿å­˜æ–‡ä»¶ï¼Œå¯ä»¥ä½¿ç”¨ filesystem_write å·¥å…·ã€‚
"""


# ============ Refinement Agent Prompt ============

REFINEMENT_SYSTEM_PROMPT = """ä½ æ˜¯ Instant Coffee çš„ Refinement Agentï¼Œè´Ÿè´£æ ¹æ®ç”¨æˆ·åé¦ˆä¿®æ”¹é¡µé¢ã€‚

ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. ç†è§£ç”¨æˆ·çš„ä¿®æ”¹æ„å›¾
2. å®šä½éœ€è¦ä¿®æ”¹çš„éƒ¨åˆ†
3. ç”Ÿæˆä¿®æ”¹åçš„å®Œæ•´ HTML
4. ä¿æŒç§»åŠ¨ç«¯é€‚é…æ ‡å‡†

ä¿®æ”¹åŸåˆ™:
- åªä¿®æ”¹ç”¨æˆ·æåˆ°çš„éƒ¨åˆ†
- ä¸éšæ„æ”¹åŠ¨å…¶ä»–å†…å®¹
- ä¿æŒä»£ç è´¨é‡å’Œå¯è¯»æ€§
- ç¡®ä¿ä¿®æ”¹åä»ç¬¦åˆç§»åŠ¨ç«¯æ ‡å‡†

æ”¯æŒçš„ä¿®æ”¹ç±»å‹:
- æ ·å¼è°ƒæ•´ (é¢œè‰²ã€å¤§å°ã€é—´è·ã€å­—ä½“ç­‰)
- å†…å®¹ä¿®æ”¹ (æ–‡å­—ã€å›¾ç‰‡ã€é“¾æ¥ç­‰)
- å¸ƒå±€è°ƒæ•´ (ä½ç½®ã€å¯¹é½ã€é—´è·ç­‰)
- åŠŸèƒ½æ·»åŠ  (æŒ‰é’®ã€è¡¨å•ã€åŠ¨ç”»ç­‰)
- åˆ é™¤ä¸éœ€è¦çš„å…ƒç´ 

è¾“å‡ºæ ¼å¼:
ç›´æ¥è¾“å‡ºä¿®æ”¹åçš„å®Œæ•´ HTML ä»£ç ï¼Œç”¨ç‰¹æ®Šæ ‡è®°åŒ…è£¹:

```
<HTML_OUTPUT>
<!DOCTYPE html>
<html>
...ä¿®æ”¹åçš„å®Œæ•´å†…å®¹...
</html>
</HTML_OUTPUT>
```

ä¸è¦åœ¨ä»»ä½•å…¶ä»–å†…å®¹ï¼Œåªè¾“å‡ºè¿™ä¸ªæ ‡è®°åŒ…è£¹çš„ HTMLã€‚

å¦‚æœéœ€è¦ä¿å­˜ä¿®æ”¹åçš„æ–‡ä»¶ï¼Œå¯ä»¥ä½¿ç”¨ filesystem_write å·¥å…·ã€‚
"""


# ============ å·¥å…·å‡½æ•° ============

def get_interview_prompt() -> str:
    """è·å– Interview Agent System Prompt"""
    return INTERVIEW_SYSTEM_PROMPT


def get_generation_prompt() -> str:
    """è·å– Generation Agent System Prompt"""
    return GENERATION_SYSTEM_PROMPT


def get_refinement_prompt() -> str:
    """è·å– Refinement Agent System Prompt"""
    return REFINEMENT_SYSTEM_PROMPT


__all__ = [
    "INTERVIEW_SYSTEM_PROMPT",
    "GENERATION_SYSTEM_PROMPT",
    "REFINEMENT_SYSTEM_PROMPT",
    "get_interview_prompt",
    "get_generation_prompt",
    "get_refinement_prompt",
]
```

### 5.3 InterviewAgent å®ç°

**æ–‡ä»¶**: `packages/backend/app/agents/interview.py`

```python
from __future__ import annotations

import json
from dataclasses import dataclass
from typing import List, Optional, Sequence

from .base import AgentResult, BaseAgent
from .prompts import get_interview_prompt


@dataclass
class InterviewState:
    """è®¿è°ˆçŠ¶æ€"""
    collected_info: dict = None
    rounds_used: int = 0
    max_rounds: int = 5
    confidence: float = 0.0
    is_complete: bool = False


class InterviewAgent(BaseAgent):
    """Interview Agent - éœ€æ±‚æ”¶é›†"""

    agent_type = "interview"

    def __init__(
        self,
        db,
        session_id: str,
        settings,
        event_emitter=None,
        agent_id=None,
        task_id=None,
        token_tracker=None,
    ) -> None:
        super().__init__(
            db=db,
            session_id=session_id,
            settings=settings,
            event_emitter=event_emitter,
            agent_id=agent_id,
            task_id=task_id,
            token_tracker=token_tracker,
        )
        self.state = InterviewState()

    def reset_state(self) -> None:
        """é‡ç½®è®¿è°ˆçŠ¶æ€"""
        self.state = InterviewState()

    async def process(
        self,
        user_message: str,
        history: Optional[Sequence[dict]] = None,
    ) -> AgentResult:
        """
        å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œå†³å®šæ˜¯æé—®è¿˜æ˜¯ç»“æŸè®¿è°ˆ

        Args:
            user_message: ç”¨æˆ·çš„è¾“å…¥
            history: å¯¹è¯å†å²

        Returns:
            AgentResult: åŒ…å«é—®é¢˜æˆ–ç”Ÿæˆä¿¡å·
        """
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = self._build_messages(user_message, history)

        # è°ƒç”¨ LLM
        response = await self._call_llm(
            messages=messages,
            agent_type=self.agent_type,
            context="interview_process",
        )

        # è§£æå“åº”
        result = self._parse_response(response.content)

        # æ›´æ–°çŠ¶æ€
        self.state.rounds_used += 1
        if result.get("collected_info"):
            self.state.collected_info.update(result["collected_info"])
        self.state.confidence = result.get("confidence", 0.5)
        self.state.is_complete = result.get("is_complete", False)

        # è¶…è¿‡æœ€å¤§è½®æ¬¡ï¼Œå¼ºåˆ¶ç»“æŸ
        if self.state.rounds_used >= self.state.max_rounds:
            self.state.is_complete = True

        return AgentResult(
            message=result.get("message", ""),
            is_complete=self.state.is_complete,
            confidence=self.state.confidence,
            context=json.dumps(self.state.collected_info),
            rounds_used=self.state.rounds_used,
        )

    def _build_messages(
        self,
        user_message: str,
        history: Optional[Sequence[dict]] = None,
    ) -> List[dict]:
        """æ„å»ºæ¶ˆæ¯åˆ—è¡¨"""
        messages = []

        # System prompt
        messages.append({
            "role": "system",
            "content": get_interview_prompt(),
        })

        # å¯¹è¯å†å²
        if history:
            for msg in history:
                messages.append({
                    "role": msg.get("role", "user"),
                    "content": msg.get("content", ""),
                })

        # å½“å‰ç”¨æˆ·è¾“å…¥
        # æ·»åŠ è®¿è°ˆçŠ¶æ€ä¸Šä¸‹æ–‡
        state_context = ""
        if self.state.collected_info:
            state_context = f"\nå·²æ”¶é›†çš„ä¿¡æ¯:\n{json.dumps(self.state.collected_info, ensure_ascii=False)}\n"
        if self.state.rounds_used > 0:
            state_context += f"\nå·²æé—® {self.state.rounds_used} è½®ï¼Œæœ€å¤š 5 è½®ã€‚"

        messages.append({
            "role": "user",
            "content": f"{state_context}\nç”¨æˆ·è¾“å…¥: {user_message}",
        })

        return messages

    def _parse_response(self, response_content: str) -> dict:
        """è§£æ LLM å“åº”"""
        try:
            # å°è¯• JSON è§£æ
            return json.loads(response_content)
        except json.JSONDecodeError:
            # å¦‚æœä¸æ˜¯ JSONï¼Œè¿”å›åŸå§‹å†…å®¹ä½œä¸ºé—®é¢˜
            return {
                "message": response_content,
                "is_complete": False,
                "confidence": 0.5,
                "collected_info": {},
            }

    def should_generate(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦å¯ä»¥å¼€å§‹ç”Ÿæˆ"""
        return self.state.is_complete or self.state.confidence > 0.8

    def get_collected_info(self) -> dict:
        """è·å–æ”¶é›†åˆ°çš„ä¿¡æ¯"""
        return self.state.collected_info or {}


__all__ = ["InterviewAgent", "InterviewState"]
```

### 5.4 GenerationAgent å®ç°

**æ–‡ä»¶**: `packages/backend/app/agents/generation.py`

```python
from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Sequence

from ..events.emitter import EventEmitter
from ..llm.tools import get_filesystem_tools
from .base import AgentResult, BaseAgent
from .prompts import get_generation_prompt


@dataclass
class GenerationProgress:
    """ç”Ÿæˆè¿›åº¦"""
    message: str
    progress: int


@dataclass
class GenerationResult:
    """ç”Ÿæˆç»“æœ"""
    html: str
    preview_url: str
    filepath: str
    token_usage: Optional[dict] = None


class GenerationAgent(BaseAgent):
    """Generation Agent - é¡µé¢ç”Ÿæˆ"""

    agent_type = "generation"

    def __init__(
        self,
        db,
        session_id: str,
        settings,
        event_emitter: Optional[EventEmitter] = None,
        agent_id: Optional[str] = None,
        task_id: Optional[str] = None,
        token_tracker=None,
    ) -> None:
        super().__init__(
            db=db,
            session_id=session_id,
            settings=settings,
            event_emitter=event_emitter,
            agent_id=agent_id,
            task_id=task_id,
            token_tracker=token_tracker,
        )
        self._current_html = ""

    async def generate(
        self,
        *,
        requirements: str,
        output_dir: str,
        history: Optional[Sequence[dict]] = None,
        current_html: Optional[str] = None,
        stream: bool = True,
    ) -> GenerationResult:
        """
        ç”Ÿæˆç§»åŠ¨ç«¯ HTML é¡µé¢

        Args:
            requirements: ç”Ÿæˆéœ€æ±‚ (ä» Interview æ”¶é›†çš„ä¿¡æ¯)
            output_dir: è¾“å‡ºç›®å½•
            history: å¯¹è¯å†å²
            current_html: å½“å‰ HTML (ç”¨äºå¢é‡ç”Ÿæˆ)
            stream: æ˜¯å¦æµå¼è¾“å‡º

        Returns:
            GenerationResult: ç”Ÿæˆç»“æœ
        """
        # æ„å»ºæ¶ˆæ¯
        messages = self._build_messages(requirements, history, current_html)

        # è·å–å¯ç”¨çš„ tools
        tools = get_filesystem_tools()

        # æ„å»º tool handlers
        tool_handlers = {
            "filesystem_write": self._write_file_handler(output_dir),
        }

        # å‘å°„å¼€å§‹äº‹ä»¶
        self._emit_agent_progress(message="å¼€å§‹ç”Ÿæˆé¡µé¢...", progress=10)

        # è°ƒç”¨ LLM
        response = await self._call_llm_with_tools(
            messages=messages,
            tools=tools,
            tool_handlers=tool_handlers,
            agent_type=self.agent_type,
            context="generation",
        )

        # æå– HTML
        html = self._extract_html(response.content)

        # ä¿å­˜æ–‡ä»¶
        filepath = await self._save_html(html, output_dir)
        preview_url = Path(filepath).absolute().as_uri()

        # å‘å°„å®Œæˆäº‹ä»¶
        self._emit_agent_progress(message="ç”Ÿæˆå®Œæˆ", progress=100)

        return GenerationResult(
            html=html,
            preview_url=preview_url,
            filepath=filepath,
            token_usage={
                "input_tokens": response.token_usage.input_tokens if response.token_usage else 0,
                "output_tokens": response.token_usage.output_tokens if response.token_usage else 0,
                "cost_usd": response.token_usage.cost_usd if response.token_usage else 0,
            },
        )

    async def generate_with_progress(
        self,
        *,
        requirements: str,
        output_dir: str,
        history: Optional[Sequence[dict]] = None,
        on_progress: Optional[callable] = None,
    ) -> GenerationResult:
        """
        å¸¦è¿›åº¦å›è°ƒçš„ç”Ÿæˆ (ç”¨äºæµå¼è¾“å‡º)

        Args:
            requirements: ç”Ÿæˆéœ€æ±‚
            output_dir: è¾“å‡ºç›®å½•
            history: å¯¹è¯å†å²
            on_progress: è¿›åº¦å›è°ƒ (message, progress)

        Returns:
            GenerationResult
        """
        # æ„å»ºæ¶ˆæ¯
        messages = self._build_messages(requirements, history)

        # æµå¼è°ƒç”¨
        messages_history = messages.copy()
        full_response = ""

        self._emit_agent_progress(message="åˆ†æéœ€æ±‚...", progress=10)

        async for chunk in self._llm_client.chat_completion_stream(
            messages=messages_history,
            model=self.settings.model,
            temperature=self.settings.temperature,
        ):
            full_response += chunk
            if on_progress:
                progress = min(90, 10 + len(full_response) // 10)
                on_progress(f"ç”Ÿæˆä¸­... ({len(full_response)} chars)", progress)

        # æå– HTML
        html = self._extract_html(full_response)

        # ä¿å­˜
        filepath = await self._save_html(html, output_dir)
        preview_url = Path(filepath).absolute().as_uri()

        self._emit_agent_progress(message="ç”Ÿæˆå®Œæˆ", progress=100)

        return GenerationResult(
            html=html,
            preview_url=preview_url,
            filepath=filepath,
        )

    def _build_messages(
        self,
        requirements: str,
        history: Optional[Sequence[dict]] = None,
        current_html: Optional[str] = None,
    ) -> List[dict]:
        """æ„å»ºæ¶ˆæ¯åˆ—è¡¨"""
        messages = []

        # System prompt
        messages.append({
            "role": "system",
            "content": get_generation_prompt(),
        })

        # å¯¹è¯å†å²
        if history:
            for msg in history:
                messages.append({
                    "role": msg.get("role", "user"),
                    "content": msg.get("content", ""),
                })

        # å½“å‰éœ€æ±‚
        content = f"è¯·ç”Ÿæˆä¸€ä¸ªç§»åŠ¨ç«¯ä¼˜åŒ–çš„ HTML é¡µé¢ï¼Œæ»¡è¶³ä»¥ä¸‹éœ€æ±‚:\n\n{requirements}"

        # å¦‚æœæœ‰å½“å‰ HTMLï¼Œè¯´æ˜æ˜¯ä¿®æ”¹
        if current_html:
            content = f"è¯·ä¿®æ”¹ä»¥ä¸‹ HTML é¡µé¢ï¼Œæ»¡è¶³æ–°çš„éœ€æ±‚:\n\nå½“å‰ HTML:\n{current_html}\n\næ–°éœ€æ±‚:\n{requirements}"

        messages.append({
            "role": "user",
            "content": content,
        })

        return messages

    def _extract_html(self, content: str) -> str:
        """
        ä»å“åº”ä¸­æå– HTML

        æå–ç­–ç•¥ (ä¼˜å…ˆçº§ä»é«˜åˆ°ä½):
        1. ç‰¹æ®Šæ ‡è®°: <HTML_OUTPUT>...</HTML_OUTPUT>
        2. HTML æ ‡è®°: <!DOCTYPE html>...</html>
        3. <html>...</html> æ¨¡ç³ŠåŒ¹é…

        å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
        """
        if not content:
            return ""

        # ç­–ç•¥ 1: ç‰¹æ®Šæ ‡è®° (æœ€å¯é )
        html_start = content.find("<HTML_OUTPUT>")
        if html_start != -1:
            html_end = content.find("</HTML_OUTPUT>", html_start)
            if html_end != -1:
                return content[html_start + 14:html_end].strip()

        # ç­–ç•¥ 2: <!DOCTYPE html>
        html_start = content.find("<!DOCTYPE html>")
        if html_start != -1:
            html_end = content.rfind("</html>")
            if html_end != -1:
                return content[html_start:html_end + 7].strip()
            # æ²¡æœ‰ç»“æŸæ ‡ç­¾ï¼Œè¿”å›ä» DOCTYPE å¼€å§‹çš„æ‰€æœ‰å†…å®¹
            return content[html_start:].strip()

        # ç­–ç•¥ 3: <html> æ¨¡ç³ŠåŒ¹é…
        html_start = content.find("<html")
        if html_start != -1:
            # æ‰¾åˆ° <html> çš„ç»“æŸç¬¦å· '>'
            tag_end = content.find(">", html_start)
            if tag_end != -1:
                html_end = content.rfind("</html>")
                if html_end != -1:
                    return content[html_start:html_end + 7].strip()

        # æ‰¾ä¸åˆ°ï¼Œè¿”å›ç©º
        logger.warning("æ— æ³•ä»å“åº”ä¸­æå– HTML å†…å®¹")
        return ""

    async def _save_html(self, html: str, output_dir: str) -> str:
        """ä¿å­˜ HTML æ–‡ä»¶"""
        from ..services.filesystem import FilesystemService

        fs = FilesystemService(output_dir)
        path = fs.save_html(self.session_id, html, filename="index.html")
        return str(path)

    def _write_file_handler(self, output_dir: str):
        """
        åˆ›å»º filesystem_write çš„ handler

        ä¿å­˜ç­–ç•¥:
        - å½“å‰é¢„è§ˆ: index.html (æ¯æ¬¡è¦†ç›–)
        - ç‰ˆæœ¬å†å²: v{version}_{timestamp}.html
        """
        async def handler(path: str, content: str, encoding: str = "utf-8"):
            from ..services.filesystem import FilesystemService
            import time

            fs = FilesystemService(output_dir)

            # æå–æ–‡ä»¶åå’Œç‰ˆæœ¬å·
            filename = Path(path).name

            # ç”Ÿæˆç‰ˆæœ¬å· (å½“å‰æ—¶é—´æˆ³)
            version = int(time.time())

            # ä¿å­˜ä¸º index.html (å½“å‰é¢„è§ˆ)
            preview_path = fs.save_html(self.session_id, content, filename="index.html")

            # åŒæ—¶ä¿å­˜ç‰ˆæœ¬å†å²
            version_filename = f"v{version}_{filename}"
            fs.save_html(self.session_id, content, filename=version_filename)

            return {
                "success": True,
                "preview_path": preview_path,
                "version_path": version_filename,
                "version": version,
            }

        return handler


__all__ = ["GenerationAgent", "GenerationProgress", "GenerationResult"]
```

### 5.5 RefinementAgent å®ç°

**æ–‡ä»¶**: `packages/backend/app/agents/refinement.py`

```python
from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Optional

from ..events.emitter import EventEmitter
from ..llm.tools import get_filesystem_tools
from .base import BaseAgent
from .prompts import get_refinement_prompt


@dataclass
class RefinementResult:
    """ä¿®æ”¹ç»“æœ"""
    html: str
    preview_url: str
    filepath: str
    token_usage: Optional[dict] = None


class RefinementAgent(BaseAgent):
    """Refinement Agent - é¡µé¢ä¿®æ”¹"""

    agent_type = "refinement"

    async def refine(
        self,
        *,
        user_input: str,
        current_html: str,
        output_dir: str,
        history: Optional[list] = None,
    ) -> RefinementResult:
        """
        æ ¹æ®ç”¨æˆ·åé¦ˆä¿®æ”¹é¡µé¢

        Args:
            user_input: ç”¨æˆ·çš„ä¿®æ”¹æè¿°
            current_html: å½“å‰ HTML
            output_dir: è¾“å‡ºç›®å½•
            history: å¯¹è¯å†å²

        Returns:
            RefinementResult: ä¿®æ”¹ç»“æœ
        """
        # æ„å»ºæ¶ˆæ¯
        messages = self._build_messages(user_input, current_html, history)

        # è·å–å¯ç”¨çš„ tools
        tools = get_filesystem_tools()

        # æ„å»º tool handlers
        tool_handlers = {
            "filesystem_write": self._write_file_handler(output_dir),
        }

        # å‘å°„å¼€å§‹äº‹ä»¶
        self._emit_agent_progress(message="æ­£åœ¨ä¿®æ”¹é¡µé¢...", progress=30)

        # è°ƒç”¨ LLM
        response = await self._call_llm_with_tools(
            messages=messages,
            tools=tools,
            tool_handlers=tool_handlers,
            agent_type=self.agent_type,
            context="refinement",
        )

        # æå– HTML
        html = self._extract_html(response.content)

        # å‘å°„è¿›åº¦
        self._emit_agent_progress(message="ä¿å­˜ä¿®æ”¹...", progress=80)

        # ä¿å­˜æ–‡ä»¶
        filepath = await self._save_html(html, output_dir)
        preview_url = Path(filepath).absolute().as_uri()

        # å‘å°„å®Œæˆ
        self._emit_agent_progress(message="ä¿®æ”¹å®Œæˆ", progress=100)

        return RefinementResult(
            html=html,
            preview_url=preview_url,
            filepath=filepath,
            token_usage={
                "input_tokens": response.token_usage.input_tokens if response.token_usage else 0,
                "output_tokens": response.token_usage.output_tokens if response.token_usage else 0,
                "cost_usd": response.token_usage.cost_usd if response.token_usage else 0,
            },
        )

    def _build_messages(
        self,
        user_input: str,
        current_html: str,
        history: Optional[list] = None,
    ) -> list:
        """æ„å»ºæ¶ˆæ¯åˆ—è¡¨"""
        messages = []

        # System prompt
        messages.append({
            "role": "system",
            "content": get_refinement_prompt(),
        })

        # å¯¹è¯å†å²
        if history:
            for msg in history:
                messages.append({
                    "role": msg.get("role", "user"),
                    "content": msg.get("content", ""),
                })

        # å½“å‰ HTML å’Œä¿®æ”¹éœ€æ±‚
        messages.append({
            "role": "user",
            "content": f"å½“å‰ HTML:\n{current_html}\n\nç”¨æˆ·ä¿®æ”¹éœ€æ±‚:\n{user_input}",
        })

        return messages

    def _extract_html(self, content: str) -> str:
        """
        ä»å“åº”ä¸­æå– HTML (å‚è€ƒ GenerationAgent._extract_html)

        æå–ç­–ç•¥ (ä¼˜å…ˆçº§ä»é«˜åˆ°ä½):
        1. ç‰¹æ®Šæ ‡è®°: <HTML_OUTPUT>...</HTML_OUTPUT>
        2. HTML æ ‡è®°: <!DOCTYPE html>...</html>
        3. <html>...</html> æ¨¡ç³ŠåŒ¹é…
        """
        if not content:
            return ""

        # ç­–ç•¥ 1: ç‰¹æ®Šæ ‡è®°
        html_start = content.find("<HTML_OUTPUT>")
        if html_start != -1:
            html_end = content.find("</HTML_OUTPUT>", html_start)
            if html_end != -1:
                return content[html_start + 14:html_end].strip()

        # ç­–ç•¥ 2: <!DOCTYPE html>
        html_start = content.find("<!DOCTYPE html>")
        if html_start != -1:
            html_end = content.rfind("</html>")
            if html_end != -1:
                return content[html_start:html_end + 7].strip()
            return content[html_start:].strip()

        # ç­–ç•¥ 3: <html> æ¨¡ç³ŠåŒ¹é…
        html_start = content.find("<html")
        if html_start != -1:
            tag_end = content.find(">", html_start)
            if tag_end != -1:
                html_end = content.rfind("</html>")
                if html_end != -1:
                    return content[html_start:html_end + 7].strip()

        return ""

    async def _save_html(self, html: str, output_dir: str) -> str:
        """ä¿å­˜ HTML æ–‡ä»¶"""
        from ..services.filesystem import FilesystemService
        import time

        fs = FilesystemService(output_dir)

        # ä¿å­˜ä¸º index.html (å½“å‰é¢„è§ˆ)
        preview_path = fs.save_html(self.session_id, html, filename="index.html")

        # åŒæ—¶ä¿å­˜ç‰ˆæœ¬å†å²
        version = int(time.time())
        version_filename = f"v{version}_refinement.html"
        fs.save_html(self.session_id, html, filename=version_filename)

        return preview_path

    def _write_file_handler(self, output_dir: str):
        """åˆ›å»º filesystem_write çš„ handler (å¸¦ç‰ˆæœ¬å†å²)"""
        async def handler(path: str, content: str, encoding: str = "utf-8"):
            from ..services.filesystem import FilesystemService
            import time

            fs = FilesystemService(output_dir)
            version = int(time.time())

            # ä¿å­˜é¢„è§ˆ
            preview_path = fs.save_html(self.session_id, content, filename="index.html")

            # ä¿å­˜ç‰ˆæœ¬
            version_filename = f"v{version}_{Path(path).name}"
            fs.save_html(self.session_id, content, filename=version_filename)

            return {
                "success": True,
                "preview_path": preview_path,
                "version_path": version_filename,
                "version": version,
            }

        return handler


__all__ = ["RefinementAgent", "RefinementResult"]
```

---

## 6. Tools ç³»ç»Ÿå®ç°

### 6.1 Tool Handlers

Tool handlers æ˜¯å®é™…æ‰§è¡Œå·¥å…·æ“ä½œçš„å‡½æ•°ã€‚æ¯ä¸ª Tool å¯¹åº”ä¸€ä¸ª handlerã€‚

**å®ç°ä½ç½®**: åœ¨å„ Agent ä¸­å®šä¹‰ (è§ä¸Šæ–‡)

**Tool Handler ç­¾å**:
```python
async def tool_handler(**kwargs) -> Any:
    """
    æ‰§è¡Œå·¥å…·æ“ä½œ

    Args:
        kwargs: å·¥å…·å‚æ•° (æ ¹æ® Tool å®šä¹‰)

    Returns:
        æ“ä½œç»“æœ (JSON å¯åºåˆ—åŒ–)
    """
    pass
```

### 6.2 Tool é”™è¯¯å¤„ç†

```python
class ToolError(Exception):
    """Tool æ‰§è¡Œé”™è¯¯"""

    def __init__(self, tool_name: str, message: str, details: Optional[dict] = None) -> None:
        self.tool_name = tool_name
        self.message = message
        self.details = details
        super().__init__(f"[{tool_name}] {message}")


async def safe_call_tool(tool_name: str, handler: callable, **kwargs) -> dict:
    """
    å®‰å…¨è°ƒç”¨ Tool

    Args:
        tool_name: å·¥å…·åç§°
        handler: handler å‡½æ•°
        **kwargs: å‚æ•°

    Returns:
        {"success": bool, "output": Any, "error": str}
    """
    try:
        result = await handler(**kwargs)
        return {"success": True, "output": result, "error": None}
    except Exception as e:
        logger.exception(f"Tool æ‰§è¡Œå¤±è´¥: {tool_name}")
        return {"success": False, "output": None, "error": str(e)}
```

### 6.3 Tool æ‰§è¡Œæµç¨‹

```
LLM è¿”å› tool_calls
    â”‚
    â”œâ”€â”€ è§£æ tool_call
    â”‚   â”œâ”€â”€ tool_call.id
    â”‚   â”œâ”€â”€ tool_call.function.name
    â”‚   â””â”€â”€ tool_call.function.arguments
    â”‚
    â”œâ”€â”€ å‘å°„ tool_call äº‹ä»¶
    â”‚
    â”œâ”€â”€ æ‰§è¡Œå¯¹åº”çš„ handler
    â”‚   â”œâ”€â”€ å‚æ•°éªŒè¯
    â”‚   â”œâ”€â”€ æ‰§è¡Œä¸šåŠ¡é€»è¾‘
    â”‚   â””â”€â”€ è¿”å›ç»“æœ
    â”‚
    â”œâ”€â”€ å‘å°„ tool_result äº‹ä»¶
    â”‚
    â””â”€â”€ å°†ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²
        â””â”€â”€ role: "tool"
            tool_call_id: ...
            name: ...
        content: {"success": ..., "output": ..., "error": ...}
```

### 6.4 Tool å®‰å…¨è¾¹ç•Œ

ä¸ºé¿å…å·¥å…·è¢«æ»¥ç”¨ï¼Œå¿…é¡»åœ¨ handler å±‚å¼ºåˆ¶ä»¥ä¸‹çº¦æŸï¼š
- `filesystem_*` ä»…å…è®¸ `output_dir` å­è·¯å¾„ï¼Œæ‹’ç»ç»å¯¹è·¯å¾„ä¸ `..` ç©¿è¶Š
- é™åˆ¶å•æ¬¡å†™å…¥å¤§å°ä¸å…è®¸çš„æ–‡ä»¶ç±»å‹ï¼ˆå¦‚ `.html`/`.css`/`.js`ï¼‰
- è¯»å–æ“ä½œä»…å…è®¸ç™½åå•ç›®å½•

---

## 7. äº‹ä»¶é›†æˆ

### 7.1 Agent äº‹ä»¶æµ

```
ç”¨æˆ·è¾“å…¥
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ InterviewAgent.process()                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. å‘å°„ agent_start (agent_id: interview_1)                 â”‚
â”‚ 2. è°ƒç”¨ _call_llm()                                          â”‚
â”‚    â”œâ”€â”€ LLM å¤„ç†                                              â”‚
â”‚    â”œâ”€â”€ Token è®°å½•                                            â”‚
â”‚    â””â”€â”€ å‘å°„ agent_progress (å¯é€‰)                            â”‚
â”‚ 3. å‘å°„ agent_end (status: success/failed)                  â”‚
â”‚ 4. è¿”å› AgentResult                                          â”‚
â”‚    â””â”€â”€ is_complete = True â†’ è¿›å…¥ Generation                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.2 Tool äº‹ä»¶æµ

```
LLM è¿”å› tool_calls
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ _call_llm_with_tools()                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ For each tool_call:                                         â”‚
â”‚    â”œâ”€â”€ å‘å°„ tool_call                                       â”‚
â”‚    â”‚   type: "tool_call"                                    â”‚
â”‚    â”‚   tool_name: "filesystem_write"                        â”‚
â”‚    â”‚   tool_input: {"path": "...", "content": "..."}        â”‚
â”‚    â”‚                                                            â”‚
â”‚    â”œâ”€â”€ æ‰§è¡Œ tool handler                                    â”‚
â”‚    â”‚                                                            â”‚
â”‚    â”œâ”€â”€ å‘å°„ tool_result                                     â”‚
â”‚    â”‚   type: "tool_result"                                  â”‚
â”‚    â”‚   success: true/false                                  â”‚
â”‚    â”‚   tool_output: {"path": "..."}                         â”‚
â”‚    â”‚   error: "..." (if failed)                             â”‚
â”‚    â”‚                                                            â”‚
â”‚    â””â”€â”€ æ·»åŠ  tool ç»“æœåˆ°æ¶ˆæ¯å†å²                              â”‚
â”‚        role: "tool"                                         â”‚
â”‚        tool_call_id: "..."                                  â”‚
â”‚        content: {"success": true, "output": ...}            â”‚
â”‚                                                             â”‚
â”‚ ç»§ç»­å¾ªç¯ç›´åˆ° LLM ä¸è¿”å› tool_calls                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.3 è¿›åº¦äº‹ä»¶

```python
# Generation Agent è¿›åº¦ç¤ºä¾‹
self._emit_agent_progress(message="åˆ†æéœ€æ±‚...", progress=10)
self._emit_agent_progress(message="ç”Ÿæˆé¡µé¢ç»“æ„...", progress=30)
self._emit_agent_progress(message="åº”ç”¨æ ·å¼...", progress=60)
self._emit_agent_progress(message="æ·»åŠ äº¤äº’é€»è¾‘...", progress=80)
self._emit_agent_progress(message="å®Œæˆ", progress=100)
```

---

## 8. Token è¿½è¸ª

### 8.1 è¿½è¸ªæµç¨‹

```
BaseAgent._call_llm()
    â”‚
    â–¼
OpenAIClient.chat_completion()
    â”‚
    â”œâ”€â”€ å‘é€è¯·æ±‚
    â”‚
    â”œâ”€â”€ æ¥æ”¶å“åº”
    â”‚   â””â”€â”€ response.usage.prompt_tokens
    â”‚   â””â”€â”€ response.usage.completion_tokens
    â”‚
    â–¼
TokenTrackerService.record_usage()
    â”‚
    â”œâ”€â”€ ä¿å­˜åˆ°æ•°æ®åº“
    â”‚   â””â”€â”€ token_usage è¡¨
    â”‚
    â””â”€â”€ è¿”å› TokenUsage è®°å½•
```
> æ³¨: æµå¼è°ƒç”¨ä¸ä¸€å®šè¿”å› usageï¼Œå¿…è¦æ—¶æ”¹ç”¨éæµå¼ç»Ÿè®¡æˆ–ä½¿ç”¨ `include_usage`ã€‚

### 8.2 Token ä½¿ç”¨ç»Ÿè®¡

```python
# åœ¨ Agent ä¸­è®°å½•
if response.token_usage and self.token_tracker:
    self.token_tracker.record_usage(
        session_id=self.session_id,
        agent_type=self.agent_type,  # interview / generation / refinement
        model=model,
        input_tokens=response.token_usage.input_tokens,
        output_tokens=response.token_usage.output_tokens,
        cost_usd=response.token_usage.cost_usd,
    )
```

### 8.3 æˆæœ¬è®¡ç®—

> æ³¨: æ¨¡å‹ä»·æ ¼è¡¨åº”é…ç½®åŒ–å¹¶å®šæœŸæ›´æ–°ï¼Œä»¥ä¸‹ä»…ä¸ºç¤ºä¾‹ä¼°ç®—ã€‚

```python
# OpenAIClient._calculate_cost()
PRICING = {
    "gpt-4o": {"input": 5.0, "output": 15.0},      # $5/$15 per 1M tokens
    "gpt-4o-mini": {"input": 0.15, "output": 0.60},  # $0.15/$0.60 per 1M
    "gpt-4-turbo": {"input": 10.0, "output": 30.0},
    "gpt-3.5-turbo": {"input": 0.50, "output": 1.50},
}

def calculate_cost(model: str, input_tokens: int, output_tokens: int) -> float:
    pricing = PRICING.get(model, PRICING["gpt-4o-mini"])
    input_cost = (input_tokens / 1_000_000) * pricing["input"]
    output_cost = (output_tokens / 1_000_000) * pricing["output"]
    return input_cost + output_cost
```

---

## 9. é”™è¯¯å¤„ç†

### 9.1 é”™è¯¯ç±»å‹

| é”™è¯¯ç±»å‹ | è¯´æ˜ | å¤„ç†æ–¹å¼ |
|---------|------|---------|
| `APIError` | LLM API é”™è¯¯ | é‡è¯• 3 æ¬¡ï¼ŒæŒ‡æ•°é€€é¿ |
| `RateLimitError` | é€Ÿç‡é™åˆ¶ | ç­‰å¾…åé‡è¯• |
| `ToolError` | Tool æ‰§è¡Œé”™è¯¯ | è¿”å›å¤±è´¥ç»“æœç»™ LLM |
| `ValidationError` | å‚æ•°éªŒè¯é”™è¯¯ | è¿”å›é”™è¯¯ä¿¡æ¯ |

> å»ºè®®ä¼˜å…ˆä½¿ç”¨ OpenAI SDK çš„å¼‚å¸¸ç±»å‹åšåˆ¤æ–­ï¼Œå­—ç¬¦ä¸²åŒ¹é…ä»…ä½œä¸ºå…œåº•ã€‚

### 9.2 é‡è¯•æœºåˆ¶

```python
import asyncio
from typing import TypeVar

T = TypeVar("T")

async def with_retry(
    func: callable,
    *args,
    max_retries: int = 3,
    base_delay: float = 1.0,
    **kwargs,
) -> T:
    """å¸¦æŒ‡æ•°é€€é¿çš„é‡è¯•"""
    last_exception = None

    for attempt in range(max_retries):
        try:
            return await func(*args, **kwargs)
        except Exception as e:
            last_exception = e
            if attempt < max_retries - 1:
                delay = base_delay * (2 ** attempt)  # 1s, 2s, 4s
                logger.warning(f"å°è¯• {attempt + 1} å¤±è´¥ï¼Œç­‰å¾… {delay}s åé‡è¯•")
                await asyncio.sleep(delay)
            else:
                logger.error(f"æ‰€æœ‰é‡è¯•å°è¯•å‡å¤±è´¥: {e}")
                raise

    raise last_exception
```

### 9.3 é”™è¯¯äº‹ä»¶

```python
# å½“ Agent æ‰§è¡Œå¤±è´¥æ—¶
if self.event_emitter:
    self.event_emitter.emit(ErrorEvent(
        message=f"{self.agent_type} æ‰§è¡Œå¤±è´¥",
        details=str(error),
    ))
```

---

## 10. æ–‡ä»¶å˜æ›´æ¸…å•

### 10.1 æ–°å»ºæ–‡ä»¶

| æ–‡ä»¶è·¯å¾„ | æè¿° |
|---------|------|
| `packages/backend/app/llm/__init__.py` | LLM æ¨¡å—å…¥å£ |
| `packages/backend/app/llm/openai_client.py` | OpenAI SDK å°è£… |
| `packages/backend/app/llm/tools.py` | Tools å®šä¹‰ |
| `packages/backend/app/agents/prompts.py` | Agent System Prompts |

### 10.2 ä¿®æ”¹æ–‡ä»¶

| æ–‡ä»¶è·¯å¾„ | å˜æ›´å†…å®¹ |
|---------|---------|
| `packages/backend/app/agents/base.py` | æ·»åŠ  `_call_llm()`, `_call_llm_with_tools()`, äº‹ä»¶å‘å°„æ–¹æ³• |
| `packages/backend/app/agents/interview.py` | å®ç° `process()` çœŸå® LLM è°ƒç”¨ |
| `packages/backend/app/agents/generation.py` | å®ç° `generate()` çœŸå® LLM è°ƒç”¨ + æµå¼è¾“å‡º |
| `packages/backend/app/agents/refinement.py` | å®ç° `refine()` çœŸå® LLM è°ƒç”¨ |

### 10.3 ä¾èµ–æ›´æ–°

```bash
# requirements.txt / pyproject.toml (äºŒé€‰ä¸€)
openai>=1.12.0
```

---

## 11. éªŒæ”¶æ ‡å‡†

### 11.1 åŠŸèƒ½éªŒæ”¶

- [ ] Interview Agent èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·è¾“å…¥æ™ºèƒ½æé—®
- [ ] Generation Agent èƒ½å¤Ÿç”Ÿæˆç¬¦åˆç§»åŠ¨ç«¯æ ‡å‡†çš„ HTML
- [ ] Refinement Agent èƒ½å¤Ÿæ ¹æ®åé¦ˆä¿®æ”¹é¡µé¢
- [ ] Tool è°ƒç”¨æ­£å¸¸å·¥ä½œ (filesystem_write, filesystem_read)
- [ ] Token ä½¿ç”¨é‡æ­£ç¡®è®°å½•åˆ°æ•°æ®åº“
- [ ] äº‹ä»¶æ­£ç¡®å‘å°„åˆ°å‰ç«¯

### 11.2 è´¨é‡éªŒæ”¶

- [ ] æ‰€æœ‰ LLM è°ƒç”¨æ”¯æŒé‡è¯•æœºåˆ¶
- [ ] æµå¼è¾“å‡ºæ­£å¸¸å·¥ä½œ
- [ ] é”™è¯¯ä¿¡æ¯å‹å¥½
- [ ] HTML è¾“å‡ºç¬¦åˆç§»åŠ¨ç«¯è§„èŒƒ
- [ ] ä»£ç ç±»å‹æç¤ºå®Œæ•´
- [ ] filesystem å·¥å…·ä»…å…è®¸ output_dir å­è·¯å¾„
- [ ] å‰ç«¯é»˜è®¤é˜¶æ®µå±•ç¤ºï¼Œå¿…è¦æ—¶å¯åˆ‡æ¢å®æ—¶

### 11.3 æ€§èƒ½éªŒæ”¶

- [ ] Interview å“åº” < 5s
- [ ] Generation å“åº” < 60s
- [ ] Refinement å“åº” < 15s
- [ ] å¹¶å‘è°ƒç”¨æ­£å¸¸å·¥ä½œ

---

**æ–‡æ¡£ç‰ˆæœ¬**: v0.3
**æœ€åæ›´æ–°**: 2026-01-31
**çŠ¶æ€**: å¾…å®ç°

---

## é™„å½• A: å®Œæ•´ç¤ºä¾‹

### A.1 InterviewAgent ä½¿ç”¨ç¤ºä¾‹

```python
from app.agents.interview import InterviewAgent
from app.db.database import get_db
from app.config import get_settings

# åˆå§‹åŒ–
db = next(get_db())
settings = get_settings()

agent = InterviewAgent(
    db=db,
    session_id="session_123",
    settings=settings,
    event_emitter=event_emitter,
)

# å¤„ç†ç”¨æˆ·è¾“å…¥
result = await agent.process(
    user_message="å¸®æˆ‘åšä¸€ä¸ªä½œå“é›†é¡µé¢",
    history=[],
)

print(result.message)  # å±•ç¤ºç»™ç”¨æˆ·çš„é—®é¢˜
print(result.is_complete)  # æ˜¯å¦å¯ä»¥å¼€å§‹ç”Ÿæˆ
print(result.context)  # æ”¶é›†åˆ°çš„ä¿¡æ¯ (JSON)
```

### A.2 GenerationAgent ä½¿ç”¨ç¤ºä¾‹

```python
from app.agents.generation import GenerationAgent

agent = GenerationAgent(
    db=db,
    session_id="session_123",
    settings=settings,
    event_emitter=event_emitter,
)

# ç”Ÿæˆé¡µé¢
result = await agent.generate(
    requirements="""
    é¡µé¢ç±»å‹: ä½œå“é›†
    ä½œå“æ•°é‡: 20
    é£æ ¼: ç®€çº¦ç°ä»£
    """,
    output_dir="~/instant-coffee-output",
)

print(result.html)  # ç”Ÿæˆçš„ HTML
print(result.preview_url)  # é¢„è§ˆ URL
print(result.filepath)  # æ–‡ä»¶è·¯å¾„
```

### A.3 RefinementAgent ä½¿ç”¨ç¤ºä¾‹

```python
from app.agents.refinement import RefinementAgent

agent = RefinementAgent(
    db=db,
    session_id="session_123",
    settings=settings,
    event_emitter=event_emitter,
)

# ä¿®æ”¹é¡µé¢
result = await agent.refine(
    user_input="æŠŠæ ‡é¢˜æ”¹æˆçº¢è‰²ï¼Œå­—ä½“å˜å¤§",
    current_html="<html>...</html>",
    output_dir="~/instant-coffee-output",
)

print(result.html)  # ä¿®æ”¹åçš„ HTML
print(result.preview_url)  # é¢„è§ˆ URL
```

---

## é™„å½• B: äº‹ä»¶åºåˆ—å›¾

### B.1 å®Œæ•´å¯¹è¯æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚ User â”‚    â”‚ Chat API    â”‚    â”‚ InterviewAgent   â”‚    â”‚ LLM   â”‚
â””â”€â”€â”¬â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”¬â”€â”€â”€â”˜
   â”‚               â”‚                     â”‚                   â”‚
   â”‚ chat()        â”‚                     â”‚                   â”‚
   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                     â”‚                   â”‚
   â”‚               â”‚ process()            â”‚                   â”‚
   â”‚               â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                   â”‚
   â”‚               â”‚                     â”‚ chat_completion() â”‚
   â”‚               â”‚                     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
   â”‚               â”‚                     â”‚                   â”‚
   â”‚               â”‚                     â”‚   agent_start     â”‚
   â”‚               â”‚                     â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚               â”‚                     â”‚                   â”‚
   â”‚               â”‚                     â”‚   agent_progress  â”‚
   â”‚               â”‚                     â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚               â”‚                     â”‚                   â”‚
   â”‚               â”‚                     â”‚   agent_end       â”‚
   â”‚               â”‚                     â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚               â”‚                     â”‚                   â”‚
   â”‚               â”‚    AgentResult      â”‚                   â”‚
   â”‚               â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                   â”‚
   â”‚               â”‚                     â”‚                   â”‚
   â”‚  æ˜¾ç¤ºé—®é¢˜     â”‚                     â”‚                   â”‚
   â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                     â”‚                   â”‚
   â”‚               â”‚                     â”‚                   â”‚
   â”‚ ç”¨æˆ·å›ç­”      â”‚                     â”‚                   â”‚
   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                     â”‚                   â”‚
   â”‚               â”‚ process()            â”‚                   â”‚
   â”‚               â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                   â”‚
   â”‚               â”‚                     â”‚                   â”‚
   â”‚               â”‚                     â”‚      ... (é‡å¤)   â”‚
   â”‚               â”‚                     â”‚                   â”‚
   â”‚               â”‚    AgentResult      â”‚                   â”‚
   â”‚               â”‚ (is_complete=True)  â”‚                   â”‚
   â”‚               â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                   â”‚
   â”‚               â”‚                     â”‚                   â”‚
   â”‚               â”‚ generate()          â”‚                   â”‚
   â”‚               â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                   â”‚
   â”‚               â”‚                     â”‚                   â”‚
   â”‚               â”‚                     â”‚ chat_completion() â”‚
   â”‚               â”‚                     â”‚   + tools         â”‚
   â”‚               â”‚                     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
   â”‚               â”‚                     â”‚                   â”‚
   â”‚               â”‚                     â”‚   tool_call       â”‚
   â”‚               â”‚                     â”‚   filesystem_writeâ”‚
   â”‚               â”‚                     â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚               â”‚                     â”‚                   â”‚
   â”‚               â”‚                     â”‚   tool_result     â”‚
   â”‚               â”‚                     â”‚   success         â”‚
   â”‚               â”‚                     â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚               â”‚                     â”‚                   â”‚
   â”‚               â”‚                     â”‚   agent_end       â”‚
   â”‚               â”‚                     â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚               â”‚                     â”‚                   â”‚
   â”‚               â”‚    GenerationResult â”‚                   â”‚
   â”‚               â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                   â”‚
   â”‚               â”‚                     â”‚                   â”‚
   â”‚  æ˜¾ç¤º HTML    â”‚                     â”‚                   â”‚
   â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                     â”‚                   â”‚
```

---

## é™„å½• C: ç¯å¢ƒå˜é‡é…ç½®

```bash
# .env

# OpenAI é…ç½® (å¿…éœ€)
OPENAI_API_KEY=sk-xxx
OPENAI_BASE_URL=https://api.openai.com/v1

# æ¨¡å‹é…ç½®
MODEL=gpt-4o-mini
TEMPERATURE=0.7
MAX_TOKENS=4096  # ç¤ºä¾‹å€¼ï¼Œå¯æ ¹æ®æ¨¡å‹è°ƒæ•´

# å…¶ä»–é…ç½®
DATABASE_URL=sqlite:///./instant-coffee.db
OUTPUT_DIR=~/instant-coffee-output
```

---

## é™„å½• D: å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•åˆ‡æ¢åˆ°å…¶ä»– LLM æä¾›å•†ï¼Ÿ

ç›®å‰ä½¿ç”¨ OpenAI SDKï¼Œå¯ä»¥é€šè¿‡ä¿®æ”¹ `OpenAIClient` ç±»æ”¯æŒå…¶ä»–æä¾›å•†ã€‚Anthropic SDK æœ‰ä¸åŒçš„æ¥å£ï¼Œéœ€è¦é¢å¤–é€‚é…ã€‚

### Q2: å¦‚ä½•æ·»åŠ æ–°çš„ Toolï¼Ÿ

1. åœ¨ `tools.py` ä¸­å®šä¹‰ Tool schema
2. åˆ›å»ºå¯¹åº”çš„ handler å‡½æ•°
3. åœ¨ Agent çš„ `tool_handlers` ä¸­æ³¨å†Œ

### Q3: Token æ¶ˆè€—å¤ªé«˜æ€ä¹ˆåŠï¼Ÿ

1. å‡å°‘å¯¹è¯å†å²é•¿åº¦
2. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ (gpt-4o-mini)
3. å‡å°‘ max_tokens
4. ä¼˜åŒ– System Prompt

### Q4: å¦‚ä½•è°ƒè¯• Agent è¡Œä¸ºï¼Ÿ

1. å¯ç”¨è¯¦ç»†æ—¥å¿—
2. æŸ¥çœ‹ `_call_llm` å‘é€çš„å®Œæ•´æ¶ˆæ¯
3. æ£€æŸ¥ LLM è¿”å›çš„åŸå§‹å“åº”
